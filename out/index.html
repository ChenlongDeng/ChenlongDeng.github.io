<!DOCTYPE html><!--TBE55YxWzlXptoDhIROQP--><html lang="en" class="scroll-smooth"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="/portrait.jpg"/><link rel="stylesheet" href="/_next/static/css/d806aa7c76c3554a.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-32b99883bf220ffe.js"/><script src="/_next/static/chunks/4bd1b696-c023c6e3521b1417.js" async=""></script><script src="/_next/static/chunks/255-102f2e5b2e3dc2ef.js" async=""></script><script src="/_next/static/chunks/main-app-6a2a3e564656d13e.js" async=""></script><script src="/_next/static/chunks/185-02e5984e85404956.js" async=""></script><script src="/_next/static/chunks/619-9168df9c2a29b74b.js" async=""></script><script src="/_next/static/chunks/47-b3e40b271709600c.js" async=""></script><script src="/_next/static/chunks/app/layout-ec54198a7940538f.js" async=""></script><script src="/_next/static/chunks/244-139d8a8c39cb0121.js" async=""></script><script src="/_next/static/chunks/681-5664a086f962ebe9.js" async=""></script><script src="/_next/static/chunks/app/page-70ea77126a5e2ba1.js" async=""></script><link rel="icon" href="/favicon.svg" type="image/svg+xml"/><link rel="dns-prefetch" href="https://google-fonts.jialeliu.com"/><link rel="preconnect" href="https://google-fonts.jialeliu.com" crossorigin=""/><link rel="preload" as="style" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/><title>Chenlong Deng</title><meta name="description" content="PhD student at the Gaoling School of Artificial Intelligence, Renmin University of China."/><meta name="author" content="Chenlong Deng (ÈÇìÁêõÈæô)"/><meta name="keywords" content="Chenlong Deng (ÈÇìÁêõÈæô),PhD,Research,Gaoling School of Artificial Intelligence
Renmin University of China"/><meta name="creator" content="Chenlong Deng (ÈÇìÁêõÈæô)"/><meta name="publisher" content="Chenlong Deng (ÈÇìÁêõÈæô)"/><meta property="og:title" content="Chenlong Deng"/><meta property="og:description" content="PhD student at the Gaoling School of Artificial Intelligence, Renmin University of China."/><meta property="og:site_name" content="Chenlong Deng (ÈÇìÁêõÈæô)&#x27;s Academic Website"/><meta property="og:locale" content="en_US"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Chenlong Deng"/><meta name="twitter:description" content="PhD student at the Gaoling School of Artificial Intelligence, Renmin University of China."/><link rel="icon" href="/favicon.svg"/><script>
              (function() {
                // Always use current system preference (no persistence)
                const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
                const effective = prefersDark ? 'dark' : 'light';
                document.documentElement.classList.add(effective);
                document.documentElement.setAttribute('data-theme', effective);
              })();
            </script><link rel="stylesheet" id="gfonts-css" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap" media="print"/><script>
              (function(){
                var l = document.getElementById('gfonts-css');
                if (!l) return;
                if (l.media !== 'all') {
                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });
                }
              })();
            </script><noscript><link rel="stylesheet" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/></noscript><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="font-sans antialiased"><div hidden=""><!--$--><!--/$--></div><div style="visibility:hidden"><nav class="fixed top-0 left-0 right-0 z-50" data-headlessui-state=""><div class="transition-all duration-300 ease-out relative bg-transparent" style="transform:translateY(-100px)"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 relative z-10"><div class="flex justify-between items-center h-16 lg:h-20"><div class="flex-shrink-0" tabindex="0"><a class="text-xl lg:text-2xl font-serif font-semibold text-primary hover:text-accent transition-colors duration-200" href="/">Chenlong Deng</a></div><div class="hidden lg:block"><div class="ml-10 flex items-center space-x-8"><div class="flex items-baseline space-x-8"><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-primary" href="/"><span class="relative z-10">About</span><div class="absolute inset-0 bg-accent/10 rounded-lg"></div></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/publications/"><span class="relative z-10">Publications</span></a></div><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div></div></div><div class="lg:hidden flex items-center space-x-2"><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div><button class="inline-flex items-center justify-center p-2 rounded-md text-neutral-600 hover:text-primary hover:bg-neutral-100 dark:hover:bg-neutral-800 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-accent transition-colors duration-200" id="headlessui-disclosure-button-_R_npdb_" type="button" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open main menu</span><div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="block h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></div></button></div></div></div></div></nav><main class="min-h-screen pt-16 lg:pt-20"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-6 bg-background min-h-screen"><div class="grid grid-cols-1 lg:grid-cols-3 gap-10"><div class="lg:col-span-1"><div class="sticky top-8" style="opacity:0;transform:translateY(20px)"><div class="w-56 h-56 mx-auto mb-5 rounded-2xl overflow-hidden shadow-lg hover:shadow-xl transition-all duration-300 hover:scale-[1.03] border-2 border-neutral-200 dark:border-neutral-700"><img alt="Chenlong Deng (ÈÇìÁêõÈæô)" width="224" height="224" decoding="async" data-nimg="1" class="w-full h-full object-cover object-[32%_center]" style="color:transparent" src="/portrait.jpg"/></div><div class="text-center mb-6"><h1 class="text-3xl font-serif font-bold text-primary mb-2 tracking-tight">Chenlong Deng (ÈÇìÁêõÈæô)</h1><p class="text-base text-accent font-semibold mb-1">Fourth-year PhD Student</p><p class="text-sm text-neutral-600 dark:text-neutral-400 whitespace-pre-line">Gaoling School of Artificial Intelligence
Renmin University of China</p></div><div class="flex flex-wrap justify-center gap-3 sm:gap-4 mb-8 relative px-2"><div class="relative"><button class="p-2 sm:p-2 transition-colors duration-200 text-neutral-600 dark:text-neutral-400 hover:text-accent" aria-label="Email"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M21.75 6.75v10.5a2.25 2.25 0 0 1-2.25 2.25h-15a2.25 2.25 0 0 1-2.25-2.25V6.75m19.5 0A2.25 2.25 0 0 0 19.5 4.5h-15a2.25 2.25 0 0 0-2.25 2.25m19.5 0v.243a2.25 2.25 0 0 1-1.07 1.916l-7.5 4.615a2.25 2.25 0 0 1-2.36 0L3.32 8.91a2.25 2.25 0 0 1-1.07-1.916V6.75"></path></svg></button></div><div class="relative"><button class="p-2 sm:p-2 transition-colors duration-200 text-neutral-600 dark:text-neutral-400 hover:text-accent" aria-label="Location"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M15 10.5a3 3 0 1 1-6 0 3 3 0 0 1 6 0Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 10.5c0 7.142-7.5 11.25-7.5 11.25S4.5 17.642 4.5 10.5a7.5 7.5 0 1 1 15 0Z"></path></svg></button></div><a href="https://scholar.google.com/citations?user=nqp_-AsAAAAJ&amp;hl=en&amp;oi=ao" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="Google Scholar"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M4.26 10.147a60.438 60.438 0 0 0-.491 6.347A48.62 48.62 0 0 1 12 20.904a48.62 48.62 0 0 1 8.232-4.41 60.46 60.46 0 0 0-.491-6.347m-15.482 0a50.636 50.636 0 0 0-2.658-.813A59.906 59.906 0 0 1 12 3.493a59.903 59.903 0 0 1 10.399 5.84c-.896.248-1.783.52-2.658.814m-15.482 0A50.717 50.717 0 0 1 12 13.489a50.702 50.702 0 0 1 7.74-3.342M6.75 15a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm0 0v-3.675A55.378 55.378 0 0 1 12 8.443m-7.007 11.55A5.981 5.981 0 0 0 6.75 15.75v-1.5"></path></svg></a><a href="https://orcid.org/0009-0004-9888-2589" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="ORCID"><svg viewBox="0 0 24 24" fill="currentColor" class="h-5 w-5" xmlns="http://www.w3.org/2000/svg"><path d="M12 0C5.372 0 0 5.372 0 12s5.372 12 12 12 12-5.372 12-12S18.628 0 12 0zM7.369 4.378c.525 0 .947.431.947.947s-.422.947-.947.947a.95.95 0 0 1-.947-.947c0-.525.422-.947.947-.947zm-.722 3.038h1.444v10.041H6.647V7.416zm3.562 0h3.9c3.712 0 5.344 2.653 5.344 5.025 0 2.578-2.016 5.025-5.325 5.025h-3.919V7.416zm1.444 1.303v7.444h2.297c3.272 0 4.022-2.484 4.022-3.722 0-2.016-1.284-3.722-4.097-3.722h-2.222z"></path></svg></a><a href="https://github.com/ChenlongDeng" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github h-5 w-5" aria-hidden="true"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg></a><a href="https://www.linkedin.com/in/chenlong-deng-bba26b377/" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="LinkedIn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-linkedin h-5 w-5" aria-hidden="true"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect width="4" height="12" x="2" y="9"></rect><circle cx="4" cy="4" r="2"></circle></svg></a></div></div></div><div class="lg:col-span-2 space-y-7 relative"><div aria-hidden="true" class="pointer-events-none absolute -top-8 -right-8 h-40 w-40 rounded-full bg-accent/10 blur-3xl animate-float-slow"></div><div aria-hidden="true" class="pointer-events-none absolute top-64 -left-10 h-44 w-44 rounded-full bg-sky-400/10 dark:bg-sky-300/10 blur-3xl animate-float-slower"></div><section id="about" class="scroll-mt-24 space-y-8 relative z-10"><section class="space-y-3" style="opacity:0;transform:translateY(20px)"><h2 class="text-[1.65rem] font-serif font-bold text-primary mb-1">About</h2><div class="motion-card surface-card rounded-2xl border border-neutral-200/80 dark:border-neutral-700/80 bg-white/85 dark:bg-neutral-800/65 backdrop-blur-sm p-4 sm:p-5 text-neutral-700 dark:text-neutral-300 leading-[1.7] text-[0.95rem] shadow-sm"><p class="mb-2.5 last:mb-0 leading-relaxed">Chenlong Deng is a fourth-year PhD student at the <a href="https://ai.ruc.edu.cn/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 hover:underline hover:text-accent-dark">Gaoling School of Artificial Intelligence</a>, <a href="https://en.ruc.edu.cn/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 hover:underline hover:text-accent-dark">Renmin University of China</a>, advised by <a href="https://scholar.google.com/citations?user=XLqF8_AAAAAJ&amp;hl=en" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 hover:underline hover:text-accent-dark">Prof. Zhicheng Dou</a>. He received his Bachelor&#x27;s degree in Computer Science from Renmin University of China in 2022.</p>
<p class="mb-2.5 last:mb-0 leading-relaxed">His research lies at the intersection of Large Language Models (LLMs) and Information Retrieval (IR), with a current focus on two directions:</p>
<ol class="list-decimal list-inside mb-2.5 space-y-1 ml-3">
<li class="mb-0.5"><strong class="font-semibold text-primary">Efficient Long-Context Language Models</strong>: exploring sparsity and compression techniques to enable LLMs to process extended contexts efficiently.</li>
<li class="mb-0.5"><strong class="font-semibold text-primary">Deep Search Agents</strong>: investigating LLM-based agentic systems for complex, multi-step information seeking.</li>
</ol>
<p class="mb-2.5 last:mb-0 leading-relaxed">His work is grounded in years of research experience in information retrieval, spanning both fundamental and applied topics.</p>
<p class="mb-2.5 last:mb-0 leading-relaxed">He is actively seeking research internship and full-time opportunities. Feel free to reach out if you are interested in collaboration.</p></div></section><section class="space-y-7" style="opacity:0;transform:translateY(20px)"><h2 class="text-[1.65rem] font-serif font-bold text-primary">Education &amp; Experience</h2><div class="space-y-4"><h3 class="text-base font-semibold text-primary flex items-center gap-2"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-[1.1rem] w-[1.1rem] text-accent"><path stroke-linecap="round" stroke-linejoin="round" d="M4.26 10.147a60.438 60.438 0 0 0-.491 6.347A48.62 48.62 0 0 1 12 20.904a48.62 48.62 0 0 1 8.232-4.41 60.46 60.46 0 0 0-.491-6.347m-15.482 0a50.636 50.636 0 0 0-2.658-.813A59.906 59.906 0 0 1 12 3.493a59.903 59.903 0 0 1 10.399 5.84c-.896.248-1.783.52-2.658.814m-15.482 0A50.717 50.717 0 0 1 12 13.489a50.702 50.702 0 0 1 7.74-3.342M6.75 15a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm0 0v-3.675A55.378 55.378 0 0 1 12 8.443m-7.007 11.55A5.981 5.981 0 0 0 6.75 15.75v-1.5"></path></svg>Education</h3><div class="space-y-4"><div class="motion-card surface-card group rounded-xl border border-neutral-200/80 dark:border-neutral-700/70 bg-white/90 dark:bg-neutral-800/70 px-4 py-3 sm:px-5 sm:py-3.5 shadow-sm" style="opacity:0;transform:translateX(-20px)"><div class="flex items-center gap-3.5"><div class="h-10 w-10 sm:h-11 sm:w-11 rounded-lg overflow-hidden bg-white ring-1 ring-neutral-200 dark:ring-neutral-700 dark:bg-neutral-800 transition-transform duration-300 group-hover:scale-[1.04]"><img alt="Renmin University of China logo" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="h-full w-full object-contain p-1.5" style="color:transparent" src="/logos/RUC.png"/></div><div class="min-w-0 flex-1 flex flex-wrap items-center justify-between gap-2"><div class="min-w-0 flex flex-wrap items-center gap-x-2 gap-y-1"><h4 class="text-[0.96rem] sm:text-[1rem] font-semibold text-primary leading-tight">Renmin University of China</h4><span class="text-neutral-400 dark:text-neutral-500">|</span><p class="text-[0.82rem] sm:text-[0.86rem] text-neutral-700 dark:text-neutral-200 font-medium">PhD in Artificial Intelligence</p><span class="text-neutral-400 dark:text-neutral-500">|</span><p class="text-[0.8rem] sm:text-[0.84rem] text-neutral-500 dark:text-neutral-400">Beijing, China</p></div><span class="text-[0.75rem] sm:text-[0.8rem] text-accent font-medium whitespace-nowrap px-2.5 py-1 rounded-full bg-accent/10">2022 - Present</span></div></div></div><div class="motion-card surface-card group rounded-xl border border-neutral-200/80 dark:border-neutral-700/70 bg-white/90 dark:bg-neutral-800/70 px-4 py-3 sm:px-5 sm:py-3.5 shadow-sm" style="opacity:0;transform:translateX(-20px)"><div class="flex items-center gap-3.5"><div class="h-10 w-10 sm:h-11 sm:w-11 rounded-lg overflow-hidden bg-white ring-1 ring-neutral-200 dark:ring-neutral-700 dark:bg-neutral-800 transition-transform duration-300 group-hover:scale-[1.04]"><img alt="Renmin University of China logo" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="h-full w-full object-contain p-1.5" style="color:transparent" src="/logos/RUC.png"/></div><div class="min-w-0 flex-1 flex flex-wrap items-center justify-between gap-2"><div class="min-w-0 flex flex-wrap items-center gap-x-2 gap-y-1"><h4 class="text-[0.96rem] sm:text-[1rem] font-semibold text-primary leading-tight">Renmin University of China</h4><span class="text-neutral-400 dark:text-neutral-500">|</span><p class="text-[0.82rem] sm:text-[0.86rem] text-neutral-700 dark:text-neutral-200 font-medium">B.Eng. in Computer Science</p><span class="text-neutral-400 dark:text-neutral-500">|</span><p class="text-[0.8rem] sm:text-[0.84rem] text-neutral-500 dark:text-neutral-400">Beijing, China</p></div><span class="text-[0.75rem] sm:text-[0.8rem] text-accent font-medium whitespace-nowrap px-2.5 py-1 rounded-full bg-accent/10">2018 - 2022</span></div></div></div></div></div><div class="space-y-4"><h3 class="text-base font-semibold text-primary flex items-center gap-2"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-[1.1rem] w-[1.1rem] text-accent"><path stroke-linecap="round" stroke-linejoin="round" d="M20.25 14.15v4.25c0 1.094-.787 2.036-1.872 2.18-2.087.277-4.216.42-6.378.42s-4.291-.143-6.378-.42c-1.085-.144-1.872-1.086-1.872-2.18v-4.25m16.5 0a2.18 2.18 0 0 0 .75-1.661V8.706c0-1.081-.768-2.015-1.837-2.175a48.114 48.114 0 0 0-3.413-.387m4.5 8.006c-.194.165-.42.295-.673.38A23.978 23.978 0 0 1 12 15.75c-2.648 0-5.195-.429-7.577-1.22a2.016 2.016 0 0 1-.673-.38m0 0A2.18 2.18 0 0 1 3 12.489V8.706c0-1.081.768-2.015 1.837-2.175a48.111 48.111 0 0 1 3.413-.387m7.5 0V5.25A2.25 2.25 0 0 0 13.5 3h-3a2.25 2.25 0 0 0-2.25 2.25v.894m7.5 0a48.667 48.667 0 0 0-7.5 0M12 12.75h.008v.008H12v-.008Z"></path></svg>Experience</h3><div class="space-y-4"><div class="motion-card surface-card group rounded-xl border border-neutral-200/80 dark:border-neutral-700/70 bg-white/90 dark:bg-neutral-800/70 px-4 py-3 sm:px-5 sm:py-3.5 shadow-sm" style="opacity:0;transform:translateX(-20px)"><div class="flex items-center gap-3.5"><div class="h-10 w-10 sm:h-11 sm:w-11 rounded-lg overflow-hidden bg-white ring-1 ring-neutral-200 dark:ring-neutral-700 dark:bg-neutral-800 transition-transform duration-300 group-hover:scale-[1.04]"><img alt="Tencent AI Lab logo" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="h-full w-full object-contain p-1.5" style="color:transparent" src="/logos/TencentAILab.png"/></div><div class="min-w-0 flex-1 flex flex-wrap items-center justify-between gap-2"><div class="min-w-0 flex flex-wrap items-center gap-x-2 gap-y-1"><h4 class="text-[0.96rem] sm:text-[1rem] font-semibold text-primary leading-tight">Tencent AI Lab</h4><span class="text-neutral-400 dark:text-neutral-500">|</span><p class="text-[0.82rem] sm:text-[0.86rem] text-neutral-700 dark:text-neutral-200 font-medium">Research Intern</p><span class="text-neutral-400 dark:text-neutral-500">|</span><p class="text-[0.8rem] sm:text-[0.84rem] text-neutral-500 dark:text-neutral-400">Shenzhen, China</p></div><span class="text-[0.75rem] sm:text-[0.8rem] text-accent font-medium whitespace-nowrap px-2.5 py-1 rounded-full bg-accent/10">Apr 2024 - Jul 2025</span></div></div></div></div></div></section><section style="opacity:0;transform:translateY(20px)"><h2 class="text-[1.65rem] font-serif font-bold text-primary mb-4 flex items-center gap-2"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5 text-accent"><path fill-rule="evenodd" d="M9 4.5a.75.75 0 0 1 .721.544l.813 2.846a3.75 3.75 0 0 0 2.576 2.576l2.846.813a.75.75 0 0 1 0 1.442l-2.846.813a3.75 3.75 0 0 0-2.576 2.576l-.813 2.846a.75.75 0 0 1-1.442 0l-.813-2.846a3.75 3.75 0 0 0-2.576-2.576l-2.846-.813a.75.75 0 0 1 0-1.442l2.846-.813A3.75 3.75 0 0 0 7.466 7.89l.813-2.846A.75.75 0 0 1 9 4.5ZM18 1.5a.75.75 0 0 1 .728.568l.258 1.036c.236.94.97 1.674 1.91 1.91l1.036.258a.75.75 0 0 1 0 1.456l-1.036.258c-.94.236-1.674.97-1.91 1.91l-.258 1.036a.75.75 0 0 1-1.456 0l-.258-1.036a2.625 2.625 0 0 0-1.91-1.91l-1.036-.258a.75.75 0 0 1 0-1.456l1.036-.258a2.625 2.625 0 0 0 1.91-1.91l.258-1.036A.75.75 0 0 1 18 1.5ZM16.5 15a.75.75 0 0 1 .712.513l.394 1.183c.15.447.5.799.948.948l1.183.395a.75.75 0 0 1 0 1.422l-1.183.395c-.447.15-.799.5-.948.948l-.395 1.183a.75.75 0 0 1-1.422 0l-.395-1.183a1.5 1.5 0 0 0-.948-.948l-1.183-.395a.75.75 0 0 1 0-1.422l1.183-.395c.447-.15.799-.5.948-.948l.395-1.183A.75.75 0 0 1 16.5 15Z" clip-rule="evenodd"></path></svg>News</h2><div class="motion-card surface-card space-y-2.5 bg-gradient-to-br from-neutral-50 to-neutral-100 dark:from-neutral-800 dark:to-neutral-900 rounded-xl p-4 shadow-sm border border-neutral-200 dark:border-neutral-700/80"><div class="flex items-start gap-2.5 pb-2.5 last:pb-0 border-b border-neutral-200 dark:border-neutral-700/70 last:border-0" style="opacity:0;transform:translateX(-10px)"><span class="text-[0.72rem] font-semibold text-accent bg-accent/10 px-2 py-1 rounded-md whitespace-nowrap mt-0.5 transition-colors duration-200">2026-02</span><p class="text-[0.9rem] text-neutral-700 dark:text-neutral-200 leading-relaxed flex-1">We released <a href="https://arxiv.org/abs/2602.10809" target="_blank" rel="noopener noreferrer" class="text-accent hover:underline underline-offset-2">DeepImageSearch</a>, defining a new paradigm for image retrieval.</p></div><div class="flex items-start gap-2.5 pb-2.5 last:pb-0 border-b border-neutral-200 dark:border-neutral-700/70 last:border-0" style="opacity:0;transform:translateX(-10px)"><span class="text-[0.72rem] font-semibold text-accent bg-accent/10 px-2 py-1 rounded-md whitespace-nowrap mt-0.5 transition-colors duration-200">2025-09</span><p class="text-[0.9rem] text-neutral-700 dark:text-neutral-200 leading-relaxed flex-1">1 paper accepted to NeurIPS 2025 üéâ Thanks to all collaborators!</p></div><div class="flex items-start gap-2.5 pb-2.5 last:pb-0 border-b border-neutral-200 dark:border-neutral-700/70 last:border-0" style="opacity:0;transform:translateX(-10px)"><span class="text-[0.72rem] font-semibold text-accent bg-accent/10 px-2 py-1 rounded-md whitespace-nowrap mt-0.5 transition-colors duration-200">2025-07</span><p class="text-[0.9rem] text-neutral-700 dark:text-neutral-200 leading-relaxed flex-1">2 papers accepted to ACL 2025, including 1 Oral üéâ Thanks to all collaborators!</p></div><div class="flex items-start gap-2.5 pb-2.5 last:pb-0 border-b border-neutral-200 dark:border-neutral-700/70 last:border-0" style="opacity:0;transform:translateX(-10px)"><span class="text-[0.72rem] font-semibold text-accent bg-accent/10 px-2 py-1 rounded-md whitespace-nowrap mt-0.5 transition-colors duration-200">2024-09</span><p class="text-[0.9rem] text-neutral-700 dark:text-neutral-200 leading-relaxed flex-1">4 papers accepted to EMNLP 2024 üéâ Thanks to all collaborators!</p></div></div></section><section style="opacity:0;transform:translateY(20px)"><div class="flex items-center justify-between mb-4"><h2 class="text-[1.5rem] font-serif font-bold text-primary flex items-center gap-2"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5 text-accent"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Selected Publications</h2><a class="text-accent hover:text-accent-dark text-[0.78rem] font-semibold transition-all duration-200 hover:underline flex items-center gap-1" href="/publications/">View All<span class="text-[0.82rem]">‚Üí</span></a></div><div class="space-y-3.5"><div class="surface-card bg-white dark:bg-neutral-900 p-5 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/DeepImageSearch.png"/></div></div><div class="flex-grow min-w-0"><h3 class="text-[0.96rem] font-semibold text-primary mb-1.5 leading-tight">DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories</h3><p class="text-[0.82rem] text-neutral-600 dark:text-neutral-300 mb-1.5 leading-relaxed"><span><span class="font-semibold text-accent ">Chenlong Deng</span>, </span><span><span class=" ">Mengjie Deng</span>, </span><span><span class=" ">Junjie Wu</span>, </span><span><span class=" ">Dun Zeng</span>, </span><span><span class=" ">Teng Wang</span>, </span><span><span class=" ">Qingsong Xie</span>, </span><span><span class=" ">Jiadeng Huang</span>, </span><span><span class=" ">Shengjie Ma</span>, </span><span><span class=" ">Changwang Zhang</span>, </span><span><span class=" ">Zhaoxiang Wang</span>, </span><span><span class=" ">Jun Wang</span>, </span><span><span class=" ">Yutao Zhu</span>, </span><span><span class=" ">Zhicheng Dou</span></span></p><div class="mb-2.5 flex items-center gap-2 flex-wrap"><span class="inline-flex items-center rounded-full bg-accent/10 text-accent px-2.5 py-1 text-[0.78rem] font-semibold">arXiv</span><a href="https://arxiv.org/abs/2602.10809" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3.5 py-1.5 rounded-full text-[0.78rem] font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3.5 w-3.5 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m2.25 0H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Paper</a></div></div></div></div><div class="surface-card bg-white dark:bg-neutral-900 p-5 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="UniGist: Towards General and Hardware-Aligned Sequence-Level Long Context Compression" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/UniGist.png"/></div></div><div class="flex-grow min-w-0"><h3 class="text-[0.96rem] font-semibold text-primary mb-1.5 leading-tight">UniGist: Towards General and Hardware-Aligned Sequence-Level Long Context Compression</h3><p class="text-[0.82rem] text-neutral-600 dark:text-neutral-300 mb-1.5 leading-relaxed"><span><span class="font-semibold text-accent ">Chenlong Deng</span>, </span><span><span class=" ">Zhisong Zhang</span>, </span><span><span class=" ">Kelong Mao</span>, </span><span><span class=" ">Shuaiyi Li</span>, </span><span><span class=" ">Tianqing Fang</span>, </span><span><span class=" ">Hongming Zhang</span>, </span><span><span class=" ">Haitao Mi</span>, </span><span><span class=" ">Dong Yu</span>, </span><span><span class=" ">Zhicheng Dou</span></span></p><div class="mb-2.5 flex items-center gap-2 flex-wrap"><span class="inline-flex items-center rounded-full bg-accent/10 text-accent px-2.5 py-1 text-[0.78rem] font-semibold">NeurIPS 2025</span><a href="https://arxiv.org/abs/2509.15763" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3.5 py-1.5 rounded-full text-[0.78rem] font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3.5 w-3.5 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m2.25 0H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Paper</a></div></div></div></div><div class="surface-card bg-white dark:bg-neutral-900 p-5 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="Large Language Models for Information Retrieval: A Survey" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/LLMIRSurvey.png"/></div></div><div class="flex-grow min-w-0"><h3 class="text-[0.96rem] font-semibold text-primary mb-1.5 leading-tight">Large Language Models for Information Retrieval: A Survey</h3><p class="text-[0.82rem] text-neutral-600 dark:text-neutral-300 mb-1.5 leading-relaxed"><span><span class=" ">Yutao Zhu</span>, </span><span><span class=" ">Huaying Yuan</span>, </span><span><span class=" ">Shuting Wang</span>, </span><span><span class=" ">Jiongnan Liu</span>, </span><span><span class=" ">Wenhan Liu</span>, </span><span><span class="font-semibold text-accent ">Chenlong Deng</span>, </span><span><span class=" ">Haonan Chen</span>, </span><span><span class=" ">Zheng Liu</span>, </span><span><span class=" ">Zhicheng Dou</span>, </span><span><span class=" ">Ji-Rong Wen</span></span></p><div class="mb-2.5 flex items-center gap-2 flex-wrap"><span class="inline-flex items-center rounded-full bg-accent/10 text-accent px-2.5 py-1 text-[0.78rem] font-semibold">TOIS 2025</span><a href="https://dl.acm.org/doi/abs/10.1145/3748304" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3.5 py-1.5 rounded-full text-[0.78rem] font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3.5 w-3.5 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m2.25 0H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Paper</a></div></div></div></div></div></section></section></div></div></div><!--$--><!--/$--></main><footer class="border-t border-neutral-200/50 bg-neutral-50/50 dark:bg-neutral-900/50 dark:border-neutral-700/50"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-6"><div class="flex flex-col sm:flex-row justify-between items-center gap-2"><p class="text-xs text-neutral-500">Last updated: <!-- -->November 18, 2025</p><p class="text-xs text-neutral-500">Personal academic website</p></div></div></footer></div><script src="/_next/static/chunks/webpack-32b99883bf220ffe.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[7558,[\"185\",\"static/chunks/185-02e5984e85404956.js\",\"619\",\"static/chunks/619-9168df9c2a29b74b.js\",\"47\",\"static/chunks/47-b3e40b271709600c.js\",\"177\",\"static/chunks/app/layout-ec54198a7940538f.js\"],\"ThemeProvider\"]\n3:I[9994,[\"185\",\"static/chunks/185-02e5984e85404956.js\",\"619\",\"static/chunks/619-9168df9c2a29b74b.js\",\"47\",\"static/chunks/47-b3e40b271709600c.js\",\"177\",\"static/chunks/app/layout-ec54198a7940538f.js\"],\"default\"]\n4:I[9766,[],\"\"]\n5:I[8924,[],\"\"]\n6:I[7923,[\"185\",\"static/chunks/185-02e5984e85404956.js\",\"619\",\"static/chunks/619-9168df9c2a29b74b.js\",\"47\",\"static/chunks/47-b3e40b271709600c.js\",\"177\",\"static/chunks/app/layout-ec54198a7940538f.js\"],\"default\"]\na:I[7150,[],\"\"]\n:HL[\"/_next/static/css/d806aa7c76c3554a.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"TBE55YxWzlXptoDhIROQP\",\"p\":\"\",\"c\":[\"\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"__PAGE__\",{}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/d806aa7c76c3554a.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"scroll-smooth\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              (function() {\\n                // Always use current system preference (no persistence)\\n                const prefersDark = window.matchMedia \u0026\u0026 window.matchMedia('(prefers-color-scheme: dark)').matches;\\n                const effective = prefersDark ? 'dark' : 'light';\\n                document.documentElement.classList.add(effective);\\n                document.documentElement.setAttribute('data-theme', effective);\\n              })();\\n            \"}}],[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/favicon.svg\",\"type\":\"image/svg+xml\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://google-fonts.jialeliu.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://google-fonts.jialeliu.com\",\"crossOrigin\":\"\"}],[\"$\",\"link\",null,{\"rel\":\"preload\",\"as\":\"style\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}],[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"id\":\"gfonts-css\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\",\"media\":\"print\",\"suppressHydrationWarning\":true}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              (function(){\\n                var l = document.getElementById('gfonts-css');\\n                if (!l) return;\\n                if (l.media !== 'all') {\\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\\n                }\\n              })();\\n            \"}}],[\"$\",\"noscript\",null,{\"children\":[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}]}]]}],[\"$\",\"body\",null,{\"className\":\"font-sans antialiased\",\"suppressHydrationWarning\":true,\"children\":[\"$\",\"$L2\",null,{\"children\":[[\"$\",\"$L3\",null,{\"items\":[{\"title\":\"About\",\"type\":\"page\",\"target\":\"about\",\"href\":\"/\"},{\"title\":\"Publications\",\"type\":\"page\",\"target\":\"publications\",\"href\":\"/publications\"}],\"siteTitle\":\"Chenlong Deng\",\"enableOnePageMode\":false}],[\"$\",\"main\",null,{\"className\":\"min-h-screen pt-16 lg:pt-20\",\"children\":[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"$L6\",null,{\"lastUpdated\":\"November 18, 2025\"}]]}]}]]}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-6 bg-background min-h-screen\",\"children\":\"$L7\"}],null,\"$L8\"]}],{},null,false]},null,false],\"$L9\",false]],\"m\":\"$undefined\",\"G\":[\"$a\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"b:I[5902,[\"185\",\"static/chunks/185-02e5984e85404956.js\",\"244\",\"static/chunks/244-139d8a8c39cb0121.js\",\"619\",\"static/chunks/619-9168df9c2a29b74b.js\",\"681\",\"static/chunks/681-5664a086f962ebe9.js\",\"974\",\"static/chunks/app/page-70ea77126a5e2ba1.js\"],\"default\"]\nc:I[470,[\"185\",\"static/chunks/185-02e5984e85404956.js\",\"244\",\"static/chunks/244-139d8a8c39cb0121.js\",\"619\",\"static/chunks/619-9168df9c2a29b74b.js\",\"681\",\"static/chunks/681-5664a086f962ebe9.js\",\"974\",\"static/chunks/app/page-70ea77126a5e2ba1.js\"],\"default\"]\ne:I[6593,[\"185\",\"static/chunks/185-02e5984e85404956.js\",\"244\",\"static/chunks/244-139d8a8c39cb0121.js\",\"619\",\"static/chunks/619-9168df9c2a29b74b.js\",\"681\",\"static/chunks/681-5664a086f962ebe9.js\",\"974\",\"static/chunks/app/page-70ea77126a5e2ba1.js\"],\"default\"]\nf:I[5743,[\"185\",\"static/chunks/185-02e5984e85404956.js\",\"244\",\"static/chunks/244-139d8a8c39cb0121.js\",\"619\",\"static/chunks/619-9168df9c2a29b74b.js\",\"681\",\"static/chunks/681-5664a086f962ebe9.js\",\"974\",\"static/chunks/app/page-70ea77126a5e2ba1.js\"],\"default\"]\n11:I[4431,[],\"OutletBoundary\"]\n13:I[5278,[],\"AsyncMetadataOutlet\"]\n15:I[4431,[],\"ViewportBoundary\"]\n17:I[4431,[],\"MetadataBoundary\"]\n18:\"$Sreact.suspense\"\nd:T406,"])</script><script>self.__next_f.push([1,"Chenlong Deng is a fourth-year PhD student at the [Gaoling School of Artificial Intelligence](https://ai.ruc.edu.cn/), [Renmin University of China](https://en.ruc.edu.cn/), advised by [Prof. Zhicheng Dou](https://scholar.google.com/citations?user=XLqF8_AAAAAJ\u0026hl=en). He received his Bachelor's degree in Computer Science from Renmin University of China in 2022.\n\nHis research lies at the intersection of Large Language Models (LLMs) and Information Retrieval (IR), with a current focus on two directions:\n1. **Efficient Long-Context Language Models**: exploring sparsity and compression techniques to enable LLMs to process extended contexts efficiently.\n2. **Deep Search Agents**: investigating LLM-based agentic systems for complex, multi-step information seeking.\n\nHis work is grounded in years of research experience in information retrieval, spanning both fundamental and applied topics.\n\nHe is actively seeking research internship and full-time opportunities. Feel free to reach out if you are interested in collaboration.\n"])</script><script>self.__next_f.push([1,"7:[\"$\",\"div\",null,{\"className\":\"grid grid-cols-1 lg:grid-cols-3 gap-10\",\"children\":[[\"$\",\"div\",null,{\"className\":\"lg:col-span-1\",\"children\":[\"$\",\"$Lb\",null,{\"author\":{\"name\":\"Chenlong Deng (ÈÇìÁêõÈæô)\",\"title\":\"Fourth-year PhD Student\",\"institution\":\"Gaoling School of Artificial Intelligence\\nRenmin University of China\",\"avatar\":\"/portrait.jpg\"},\"social\":{\"email\":\"dengchenlong@ruc.edu.cn\",\"location\":\"Beijing, China\",\"location_url\":\"https://maps.google.com\",\"location_details\":[\"Gaoling School of Artificial Intelligence,\",\"Renmin University of China, Beijing, China\"],\"google_scholar\":\"https://scholar.google.com/citations?user=nqp_-AsAAAAJ\u0026hl=en\u0026oi=ao\",\"orcid\":\"https://orcid.org/0009-0004-9888-2589\",\"github\":\"https://github.com/ChenlongDeng\",\"linkedin\":\"https://www.linkedin.com/in/chenlong-deng-bba26b377/\"},\"features\":{\"enable_likes\":true,\"enable_one_page_mode\":false}}]}],[\"$\",\"div\",null,{\"className\":\"lg:col-span-2 space-y-7 relative\",\"children\":[[\"$\",\"div\",null,{\"aria-hidden\":\"true\",\"className\":\"pointer-events-none absolute -top-8 -right-8 h-40 w-40 rounded-full bg-accent/10 blur-3xl animate-float-slow\"}],[\"$\",\"div\",null,{\"aria-hidden\":\"true\",\"className\":\"pointer-events-none absolute top-64 -left-10 h-44 w-44 rounded-full bg-sky-400/10 dark:bg-sky-300/10 blur-3xl animate-float-slower\"}],[[\"$\",\"section\",\"about\",{\"id\":\"about\",\"className\":\"scroll-mt-24 space-y-8 relative z-10\",\"children\":[[[\"$\",\"$Lc\",\"about\",{\"content\":\"$d\",\"title\":\"About\"}],[\"$\",\"$Le\",\"education_experience\",{\"education\":[{\"degree\":\"PhD in Artificial Intelligence\",\"institution\":\"Renmin University of China\",\"period\":\"2022 - Present\",\"location\":\"Beijing, China\",\"description\":\"Pursuing doctoral studies in artificial intelligence, with a focus on machine learning and intelligent systems.\",\"logo\":\"/logos/RUC.png\",\"logoAlt\":\"Renmin University of China logo\"},{\"degree\":\"B.Eng. in Computer Science\",\"institution\":\"Renmin University of China\",\"period\":\"2018 - 2022\",\"location\":\"Beijing, China\",\"description\":\"Undergraduate training in computer science.\",\"logo\":\"/logos/RUC.png\",\"logoAlt\":\"Renmin University of China logo\"}],\"experience\":[{\"title\":\"Research Intern\",\"organization\":\"Tencent AI Lab\",\"period\":\"Apr 2024 - Jul 2025\",\"location\":\"Shenzhen, China\",\"description\":\"Worked on AI research and engineering projects, including model development, experimentation, and evaluation.\",\"logo\":\"/logos/TencentAILab.png\",\"logoAlt\":\"Tencent AI Lab logo\"}],\"title\":\"Education \u0026 Experience\"}],[\"$\",\"$Lf\",\"news\",{\"items\":[{\"date\":\"2026-02\",\"content\":\"We released [DeepImageSearch](https://arxiv.org/abs/2602.10809), defining a new paradigm for image retrieval.\"},{\"date\":\"2025-09\",\"content\":\"1 paper accepted to NeurIPS 2025 üéâ Thanks to all collaborators!\"},{\"date\":\"2025-07\",\"content\":\"2 papers accepted to ACL 2025, including 1 Oral üéâ Thanks to all collaborators!\"},{\"date\":\"2024-09\",\"content\":\"4 papers accepted to EMNLP 2024 üéâ Thanks to all collaborators!\"}],\"title\":\"News\"}],\"$L10\"],false,false,false]}]]]}]]}]\n"])</script><script>self.__next_f.push([1,"8:[\"$\",\"$L11\",null,{\"children\":[\"$L12\",[\"$\",\"$L13\",null,{\"promise\":\"$@14\"}]]}]\n9:[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$L15\",null,{\"children\":\"$L16\"}],null],[\"$\",\"$L17\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$18\",null,{\"fallback\":null,\"children\":\"$L19\"}]}]}]]}]\n"])</script><script>self.__next_f.push([1,"1a:I[2597,[\"185\",\"static/chunks/185-02e5984e85404956.js\",\"244\",\"static/chunks/244-139d8a8c39cb0121.js\",\"619\",\"static/chunks/619-9168df9c2a29b74b.js\",\"681\",\"static/chunks/681-5664a086f962ebe9.js\",\"974\",\"static/chunks/app/page-70ea77126a5e2ba1.js\"],\"default\"]\n"])</script><script>self.__next_f.push([1,"10:[\"$\",\"$L1a\",\"featured_publications\",{\"publications\":[{\"id\":\"deng2026deepimagesearch\",\"title\":\"DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories\",\"authors\":[{\"name\":\"Chenlong Deng\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Mengjie Deng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Junjie Wu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Dun Zeng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Teng Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Qingsong Xie\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiadeng Huang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Shengjie Ma\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Changwang Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhaoxiang Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jun Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yutao Zhu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhicheng Dou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2026,\"month\":\"2\",\"type\":\"preprint\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$10:props:publications:0:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"arXiv preprint arXiv:2602.10809\",\"conference\":\"\",\"url\":\"https://arxiv.org/abs/2602.10809\",\"abstract\":\"\",\"description\":\"\",\"selected\":true,\"preview\":\"DeepImageSearch.png\",\"bibtex\":\"@misc{deng2026deepimagesearch,\\n  title = {DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories},\\n  author = {Chenlong Deng and Mengjie Deng and Junjie Wu and Dun Zeng and Teng Wang and Qingsong Xie and Jiadeng Huang and Shengjie Ma and Changwang Zhang and Zhaoxiang Wang and Jun Wang and Yutao Zhu and Zhicheng Dou},\\n  year = {2026},\\n  month = {feb},\\n  journal = {arXiv preprint arXiv:2602.10809},\\n  url = {https://arxiv.org/abs/2602.10809}\\n}\"},{\"id\":\"deng2025unigist\",\"title\":\"UniGist: Towards General and Hardware-Aligned Sequence-Level Long Context Compression\",\"authors\":[{\"name\":\"Chenlong Deng\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhisong Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Kelong Mao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Shuaiyi Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Tianqing Fang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Hongming Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Haitao Mi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Dong Yu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhicheng Dou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"12\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$10:props:publications:1:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"NeurIPS 2025\",\"url\":\"https://arxiv.org/abs/2509.15763\",\"abstract\":\"\",\"description\":\"\",\"selected\":true,\"preview\":\"UniGist.png\",\"bibtex\":\"@inproceedings{deng2025unigist,\\n  title = {UniGist: Towards General and Hardware-Aligned Sequence-Level Long Context Compression},\\n  author = {Chenlong Deng and Zhisong Zhang and Kelong Mao and Shuaiyi Li and Tianqing Fang and Hongming Zhang and Haitao Mi and Dong Yu and Zhicheng Dou},\\n  year = {2025},\\n  month = {dec},\\n  booktitle = {NeurIPS 2025},\\n  url = {https://arxiv.org/abs/2509.15763}\\n}\"},{\"id\":\"zhu2025llm4irsurvey\",\"title\":\"Large Language Models for Information Retrieval: A Survey\",\"authors\":[{\"name\":\"Yutao Zhu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Huaying Yuan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Shuting Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiongnan Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Wenhan Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chenlong Deng\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Haonan Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zheng Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhicheng Dou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ji-Rong Wen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$10:props:publications:2:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"ACM Transactions on Information Systems\",\"conference\":\"\",\"pages\":\"1--54\",\"url\":\"https://dl.acm.org/doi/abs/10.1145/3748304\",\"abstract\":\"\",\"description\":\"\",\"selected\":true,\"preview\":\"LLMIRSurvey.png\",\"bibtex\":\"@article{zhu2025llm4irsurvey,\\n  title = {Large Language Models for Information Retrieval: A Survey},\\n  author = {Yutao Zhu and Huaying Yuan and Shuting Wang and Jiongnan Liu and Wenhan Liu and Chenlong Deng and Haonan Chen and Zheng Liu and Zhicheng Dou and Ji-Rong Wen},\\n  year = {2025},\\n  journal = {ACM Transactions on Information Systems},\\n  pages = {1--54},\\n  url = {https://dl.acm.org/doi/abs/10.1145/3748304}\\n}\"}],\"title\":\"Selected Publications\",\"enableOnePageMode\":false}]\n"])</script><script>self.__next_f.push([1,"16:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n12:null\n"])</script><script>self.__next_f.push([1,"1b:I[622,[],\"IconMark\"]\n"])</script><script>self.__next_f.push([1,"14:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Chenlong Deng\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"PhD student at the Gaoling School of Artificial Intelligence, Renmin University of China.\"}],[\"$\",\"meta\",\"2\",{\"name\":\"author\",\"content\":\"Chenlong Deng (ÈÇìÁêõÈæô)\"}],[\"$\",\"meta\",\"3\",{\"name\":\"keywords\",\"content\":\"Chenlong Deng (ÈÇìÁêõÈæô),PhD,Research,Gaoling School of Artificial Intelligence\\nRenmin University of China\"}],[\"$\",\"meta\",\"4\",{\"name\":\"creator\",\"content\":\"Chenlong Deng (ÈÇìÁêõÈæô)\"}],[\"$\",\"meta\",\"5\",{\"name\":\"publisher\",\"content\":\"Chenlong Deng (ÈÇìÁêõÈæô)\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"Chenlong Deng\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"PhD student at the Gaoling School of Artificial Intelligence, Renmin University of China.\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:site_name\",\"content\":\"Chenlong Deng (ÈÇìÁêõÈæô)'s Academic Website\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:title\",\"content\":\"Chenlong Deng\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:description\",\"content\":\"PhD student at the Gaoling School of Artificial Intelligence, Renmin University of China.\"}],[\"$\",\"link\",\"14\",{\"rel\":\"icon\",\"href\":\"/favicon.svg\"}],[\"$\",\"$L1b\",\"15\",{}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"19:\"$14:metadata\"\n"])</script></body></html>